# Automatic-Speech-Recognition-Research

This research proposes a robust sequence-to-sequence model archi- tecture for speech recognition tasks, incorporating convolutional and recurrent neural networks (CNN and RNN) with attention mechanisms. The encoder utilizes a hybrid approach of residual convolutional blocks (RCNN) and bidirectional Long Short-Term Memory (BLSTM) networks to extract hierarchical temporal and spectral features from input sequences efficiently. The decoder employs an attention-augmented LSTM with Luong Attention, dy- namically focusing on relevant parts of the encoded representation during sequence generation. To enhance generalization and robust- ness, during data preprocessing, data augmentation techniques such as phase randomization and time-frequency masking are integrated. Features are further refined using Mel-Frequency Cepstral Coeffi- cients (MFCC) and Filter Banks (FBANK) combined with delta and delta-delta coefficients. The model is trained on the TIMIT dataset and achieves a Phoneme Error Rate (PER) of 16.7%, demonstrating the effectiveness of the proposed architecture and preprocessing strategies in capturing complex phoneme patterns and improving recognition performance.
